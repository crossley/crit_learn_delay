\documentclass[12pt]{article}
% Language and font
\usepackage[english]{babel}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
% Bibliography with APA style
\usepackage[style=apa,sortcites=true,sorting=nyt,backend=biber]{biblatex}
\DeclareLanguageMapping{american}{american-apa}
\addbibresource{sample.bib}
% Layout and formatting
\usepackage[margin=1in]{geometry}
\usepackage{setspace}
\setstretch{1.0}
\usepackage{parskip} % disables indentation and adds vertical space between paragraphs
\setlength{\parindent}{0pt}
% Math and symbols
\usepackage{amsmath}
\usepackage{mathrsfs}
\usepackage{gensymb}
% Graphics and floats
\usepackage{graphicx}
\usepackage{float}
\usepackage{caption}
% Optional: nicer font
\usepackage{lmodern}
% Turn off section numbers
\setcounter{secnumdepth}{0}

\title{Response to Reviews: \\ \vspace{1cm}
    Criterial Learning and Feedback Delay: Insights from
Computational Models and Behavioral Experiments}
\author{
    Matthew J. Crossley, 
    Benjamin O. Pelzer,
    F. Gregory Ashby
}
\date{}

\begin{document}

\maketitle 

\section{Reviewer 1}

\subsection{Reviewer comment}
The research question of the study is to investigate the
effect of delay in criterion learning in category learning.
Experiment 1 shows that in a continuous criterion learning
task with known relevant dimension delaying the feedback but
not increasing the ITI decreases performance. In other
words, delaying feedback makes it more difficult to learn
the position of a continuous criterion. Experiment 2 uses
many binary dimensions in the category learning task and
participants' goal is to learn the binary dimension. In this
task, delaying feedback does not decrease performance. The
researchers simulate the performance of three different
models and show that a model that assumes the criterion is
maintained in working memory with information drifting
across time cannot predict the observed effect. Two models
that make different assumption can predict the observed
pattern.

\subsection{Author response}
We thank the reviewer for their careful reading of our
manuscript and their thoughtful comments. We address each of
the reviewer's points in turn below.

\subsection{Reviewer comment}
I am no expert in category learning, so cannot adequately
judge the novelty of the observed results. Given this
caveat, I really enjoyed reading the manuscript and felt it
made a convincing argument using a rigorous and
state-of-the-art methodology. Using parameter-space
partitioning is an excellent and sadly underutilised way of
comparing models. It goes much beyond the typical approach
of comparing models in terms of their model fit and instead
allows deriving qualitative differences between the
candidate models. In addition, the two experiments are well
designed and executed and provide the necessary empirical
basis on which the models are compared. The paper is also
overall clearly written. Taken together, I highly recommend
this paper to be published after addressing a number of
mostly minor concerns detailed below. My main issues with
the current version relate to the presentation of the models
and modelling results and whether the space of possible
models is sufficiently explored.

\subsection{Author response}
We are grateful for the reviewer's positive assessment of
our work and for their constructive suggestions.

\subsection{Reviewer comment}
1. Exploration of model space: When reading the description
of the models it was not entirely clear to me whether the
three presented models explore the space of possible models
to a sufficient degree or whether there might be some models
missing from consideration that could change the overall
message. For example, from a theoretical point of view the
main difference between the time-dependent drift model and
the delay-sensitive learning model is that for the latter
"the criterion remains stable over time and does not drift".
However, when looking at the model descriptions more aspects
seem to differ than solely the presence of drift, namely the
presence of the feedback delay in the updating rule (Eq. 2
vs. 6). Is the change in the updating rule necessary for the
models to just differ in the noise in the criterion or is it
part of it? I am not sure if I just do not understand the
difference or if a hybrid model with drift and feedback
delay in the updating rule is possible. Likewise, are any of
the assumption in the reinforcement-learning model, which
seems to differ quite a bit from the previous models,
somewhat arbitrary or are these always the only possible
choices?

I am not asking for an extensive exploration of the model
space using additional simulations, but some descriptions of
the extent to which the presented models explore the
possible model space or if there are other possible models
that could potentially lead to different results is needed.

\subsection{Author response}
We thank the reviewer for raising this important point. We
have used this as an opportunity more systematically explore
the classes of possible models.  In particular, we explore
two classes of models: one class that assumes a criterion is
learned and one class that assumes SR learning with no
criterion.  Within the criterial-learning class we explore
three different binary assumptions: perceptual drift (yes or
no), criterial drift (yes or no), and delay-sensitive
updating (yes or no).  Within the SR learning class there is
no criterion to drift, but we do explore perceptual drift
and delay-sensitive feedback (yes or no on each). 

\subsubsection{Reviewer comment}
2. Simulation results figures: There were some aspects of
the results figures for the simulation that I found
difficult to understand:

(a) Even after reading the full paper and thinking about it
for a while, I do not understand what panels B are supposed
to show. Does this only show the subset of the parameter
space in which one of the two relevant patterns was
observed? If not, shouldn't there be data points everywhere?

(b) I do not understand why in Figure 1, for example, panel
C shows some cases in which the predicted qualitative
pattern is either "control impaired" or "other", but there
is no corresponding bar in panel A.

(c) The bars in Figure 2A are smaller than in Figures 1A and
3A. Why?

\subsection{Author response}
With the new approach to model exploration, we have heavily 
revised the simulation results figures.  We hope that the
new figures are clearer and address the reviewer's concerns.

\subsubsection{Reviewer comment}
3. Calling the simulation Experiment 1: I found it extremely
confusing that the simulation using parameter-space
partitioning was called "Experiment 1". If I read
"Experiment" I expect to see some empirical data obtained
from participants and not solely a simulation. I suggest
renaming this section to something clearer. For example
"Simulation Study" or "Parameter-Space Partitioning Across
Models".

\subsection{Author response}
We thank the reviewer for this suggestion. We have made this
change in the revision.

\subsubsection{Reviewer comment}
4. Outliers in Exp. 2:

- While I think the argument for removing outliers in Exp. 2
is fine, the actual criterion defining an outlier is not
actually given (which feels a bit ironic in a paper about
criteria).

- How do the results look with these outliers included? Does
the difference between condition vanish? Adding a footnote
or so should be enough.

- The fact that some participants did not even learn a
single criterion feels very surprising to me. Is this common
in such experiments? Please add a sentence embedding this
finding within your general experience running such
experiments.

\subsection{Author response}
We thank the reviewer for the opportunity to clarify our
method. Outliers were identified on the basis of the Gaussian
mixture model fit to the trials-to-criterion data. We now
say ``Participants that were deemed to most likely belong in
the low-performance Gaussian by this analysis were excluded
from further analysis.''

\subsubsection{Reviewer comment}
Minor points:

- Does Eq. 13 show results for trial n or n + 1? The text
and equation are inconsistent.

- The description regarding the randomisation of Experiment
2 on page 12 (paragraph before ``Procedure'') seems to be
inconsistent with Figure 4. Is the dimension picked at
random for each trial or always the same for certain problem
numbers (as implied by Figure 4)?

- In figure 4, the top says Problems "1 thru 13", but only
shows 7 problems.

\subsection{Author response}
We thank the reviewer for catchign these points. All are
corrected or revised for greater clarity in the revision.

\section{Reviewer 2}

\subsubsection{Reviewer comment}
Your action editor asked me if I could provide a review, as
they have only been able to secure one reviewer after a
considerable delay. Although I am editor in chief for the
journal, I am acting in the role of a reviewer in this case.
The action editor retains full authority to determine the
decision outcome for this manuscript.

\subsection{Author response}
We thank the editor in chief for acting as a reviewer and for
their careful reading of our manuscript and their thoughtful
comments. We address each of the editor's points in turn
below.

\subsubsection{Reviewer comment}
This manuscript reports a sophisticated approach to an
important problem. I especially like the use of mathematical
models.

\subsection{Author response}
We are glad the reviewer viewed our approach favourably.

\subsubsection{Reviewer comment}
I was concerned that the models seem to have multiple
varying features. It might be more useful to isolate
specific processes while holding other model elements
constant. Based on the introduction, I was not clear on the
theoretical motivation for comparing the time-dependent
drift and delay sensitive learning models, as the latter did
not seem to be just a version of the former but with delay
as a relevant factor. Some additional justification and/or
adjustments to the tested set of models is needed.

\subsection{Author response}
We thank the reviewer for raising this important point. In
the revision we more systematically explore the classes of
possible models.  In particular, we explore two classes of
models: one class that assumes a criterion is learned and
one class that assumes SR learning with no criterion.
Within the criterial-learning class we explore three
different binary assumptions: perceptual drift (yes or no),
criterial drift (yes or no), and delay-sensitive updating
(yes or no).  Within the SR learning class there is no
criterion to drift, but we do explore perceptual drift and
delay-sensitive feedback (yes or no on each). The motivation
for these models and parameter choices is now more fully
explained in the revision.

\subsubsection{Reviewer comment}
Despite these concerns about which models are the most
relevant to compare, I was impressed by the simulation
strategy and the focus on finding ``signature'' predictions
that distinguish different pairs of models.

\subsection{Author response}
We are glad the reviewer viewed our simulation strategy
favourably.

\subsubsection{Reviewer comment}
I think it makes more sense to have a prediction simulation
section and Experiments 1 and 2 rather than to consider the
simulation the first experiment.

\subsection{Author response}
We thank the reviewer for this suggestion. We have made this
change in the revision.

\subsubsection{Reviewer comment}
The data in Figure 6 can likely be fit much more closely by
an ExGaussian distribution than either the 1 or 2 component
Gaussian mixtures that are shown.

\subsection{Author response}
We thank the reviewer for this suggestion. We now estimate
both a single-component truncated ExGaussian and a
two-component truncated ExGaussian mixture via MLE with
proper normalization on the tasks bounded support $[9,
512]$, and compared them via AIC, BIC, and a parametric
bootstrap likelihood-ratio test (LRT).  The results from
moving from the  mixture of Gaussians used in our orginal
submission to the truncated ExGaussian mixture used in the
revision does not change the qualitative pattern of our
results in any way. However, we agree that the ExGaussian
and ExGaussian mixture better capture the skewed and bounded
distribution of our trials-to-criterion data so we have
adopted it in the revision. 

\subsubsection{Reviewer comment}
Replication the criterial learning results from the
currently-labeled Experiment 2 seems like a good idea given
the fairly low sample sizes per condition.

\subsection{Author response}
We agree with the reviewer that a replication would be
valuable but feel the data and models currently presented
represent a suitable contribution to the field as is. For
this reason, paired with the time and resource costs of
conducting a replication, we have not pursued this in the
revision.

\subsubsection{Reviewer comment}
Did stimuli remain on the screen across the feedback delay?
I don’t think so. If not, what happens if the stimulus
reappears at the time of feedback? I could imagine this
would eliminate or dramatically reduce the negative effect
of feedback delay. If so, is that consistent with the
conclusions? That is, would the models that predict a
deleterious effect of feedback delay also predict that it
should be eliminated with ``reminder'' stimuli at the time of
feedback?

\subsection{Author response}
We thank the reviewer for the opportunity to clarify the
design of our experiment. As shown in Fig. 5 and in Fig. 8
the stimulus was presented up until a response was made. At
this time the stimulus was removed and replaced by a noise
mask until feedback was delivered. We also thank the
reviewer for raising the interesting question of whether the
negative effect of feedback delay would be eliminated if the
stimulus were to reappear at the time of feedback. We agree
that this would be an interesting manipulation to explore in
future work. However, we believe that their are many
interesting followup questions that could be explored in
future work and that the current manuscript represents a
suitable contribution to the field as is.

\subsubsection{Reviewer comment}
For the currently-labeled Experiment 3, reporting
fail-to-reject results for t-tests is not a valid method for
establishing that feedback delay has no effect. This is
especially a concern given the low participant count per
condition and the fact that empirical effects go in the
``right'' direction (lower performance for delayed feedback).
A different inferential approach is needed, and one
possibility is reporting confidence intervals over effect
size. Your conclusions rely on demonstrating that you can
rule out meaningfully large effects (i.e., only negligibly
small effect sizes are included in the confidence
intervals). I suspect that the intervals will be wide and
include a range of fairly big effects, of the same magnitude
as the effects reported in Experiment 1. As such, this
experiment does not establish that the effects of feedback
delay are specific to a criterion learning task.

\subsection{Author response}
TODO: Greg, I'd appreciate your take on this comment.

I ran an equivalence test, and as the reviewer suspected,
the data are inconclusive. I also have Experiment 2 data
from an earlier semester. It shows the same pattern (though
with fewer participants and more noise), but I left it out
to avoid mixing cohorts. Just out of curiosity, I combined
it with the current data and reran the analysis, but the
results remain inconclusive. So I don’t think we can make a
strong claim that Exp 2 shows no effect of delay --- at
least not with this method.

I also tried testing whether the delay effect is larger in
Exp 1 than Exp 2 (a difference-of-differences approach), but
that too was inconclusive.

One possible way to address this is to add data from another
archived experiment, where we examined criterion learning
with a numerical Stroop dual task. Those data (n=30) showed
no effect of the dual task. Including this might add some
empirical value.

Thoughts?

\subsubsection{Reviewer comment}
I recommend running a bigger replication study that combines
both tasks to demonstrate the claimed interaction (feedback
delay impairs performance only for the criterion learning
task). The replication study should be preregistered with a
sample size justification based on the interaction effect
defined by the across-experiment comparison in the current
data. Adding a ``stimulus reminder at feedback'' condition
to this new study might be helpful in clarifying the
mechanisms producing the delay effect.

Signed,
Jeff Starns

\subsection{Author response}
We thank the reviewer for their careful reading of our
manuscript and their thoughtful comments overall. Here, we
agree that a replication / extension study would be valuable
but feel the paper as is represents a suitable contribution
to the field.

\section*{Action Editor}

\subsubsection{Reviewer comment}
The topic is made very clear in the first paragraph and
notes the need for investigating how criterions are learned
in decision models.

\subsection{Author response}
We thank the reviewer for their positive assessment of our
work.

\subsubsection{Reviewer comment}
The final paragraph of the introduction summarizes the
experiments. This seems more appropriate in the abstract,
given that no information about the experiments are
presented in the abstract.

\subsection{Author response}
We thank the reviewer for this suggestion. We have added
more detail about the experiments to the abstract, but have
elected to retain the summary of the experiments in the
introduction as we feel it helps orient the reader to the
structure of the paper.

\subsubsection{Reviewer comment}
Equation 1 needs some explanation, as this implies that the
decision Rn is made immediately on stimulus presentation. In
other words, Rn is not an outcome of some time-extended
cognitive process. Am I misunderstanding equation 1?

\subsection{Author response}
We thank the reviewer for raining this point and for the
opportunity to clarify. In essence the reviewer is correct.
Equation 1 indicates that the response is made immediately
upon stimulus presentation. We therefore are not explicitly
modelling the time-extended cognitive processes that govern
evidence accumulation and the corresponding response time.
We now say this explicitly in the revision.

\subsubsection{Reviewer comment}
What was the reason behind the choice to have equation 2
only be sensitive to negative feedback? How is ``optimal''
defined in the current case, given that response time does
not factor in the update rule?

\subsection{Author response}
We thank the reviewer for the opportunity to clarify this
point. The rationale behind the choice to have equation 2
only be sensitive to negative feedback is that if the
response is correct, then the observer has effectively
gained zero information about how their criterion should be
modified. Also, in this case otimal refers to the true
criterion value that the observer is trying to learn. We now
say this explicitly in the revision.

\subsubsection{Reviewer comment}
The use of parameter space partitioning is sensible given
the many differences among the three models. However, PSP
operates on non-stochastic models. In addition, the way you
have approached PSP is not how the original PSP is
implemented (i.e., using MCMC). This should be mentioned
explicitly to avoid confusion.

\subsection{Author response}
We thank the reviewer for raising this important point. We
have revised the manuscript accordingly.

\subsubsection{Reviewer comment}
The three-dimensional plots in figures 1B, 2B, and 3B do not
come out right on 2D. You could improve these by having 2D
projections on the planes. Better still, three boxplots
would be more appropriate as there is no additional
information by using 3D plots.

\subsection{Author response}
We thank the reviewer for this suggestion. We have heavily
revised the simulation results figures. We hope that the new
figures are clearer and address the reviewer's concerns.

\subsubsection{Reviewer comment}
Although not a focus in this manuscript, I wondered whether
the models are able to account for finer-grained data
patterns such as complete RT distributions and
speed-accuracy tradeoff functions.

\subsection{Author response}
We agree that these are interesting questions. The models in
their current form do not account for RT distributions or
speed-accuracy tradeoff functions. However, we believe that
this could be a fruitful avenue for future work. 

\subsubsection{Reviewer comment}
The experiments are numbered 2 and 3, instead of 1 and 2.
The models were following the methods of experiment 1, which
is missing. Experiment 1 is also mentioned in the discussion
separately with experiment 2. It looks like experiment 1 has
been deleted from this manuscript, but the model simulations
and discussion still refer to it.

\subsection{Author response}
We thank the reviewer for catching this confusion. No study
was deleted from the manuscript. Rather, whether or not the
computational modelling section should be considered an
``experiment'' was a point of oscillation when writing. We
hope our revision clarifies this point.

\subsubsection{Reviewer comment}
In figure 6, it is clear that the participants form two
subcohorts. However, the use of a Gaussian seems odd given
that there can not be any negative values for
trials-to-criterion. Given that the authors model this
frequency histogram to exclude participants, more
appropriate distributions, such as Poisson should be used.

\subsection{Author response}
We thank the reviewer for this suggestion. We now estimate
both a single-component truncated ExGaussian and a
two-component truncated ExGaussian mixture via MLE with
proper normalization on the tasks bounded support $[9,
512]$.

\subsubsection{Reviewer comment}
Given the experimental results, what is the added benefit of
the three models? Experiment 2 is the critical experiment
showing that 3.5-0.5 leads to more trials-to-criterion than
0.5-3.5 or 0.5-0.5. The models do not provide any detailed
understanding of mechanisms and they are not fitted to
actual data. The general message from the experiments is
that any updating rule should be sensitive to feedback
delay. The models do not go beyond this.

\subsection{Author response}
Thank you for raising this but we respectfully disagree that
our models do not provide a detailed understanding of
mechanism. To the contrary, our models all offer detailed
mechanistic accounts for how criterial learning may occur in
cognition. They articulate that delay effects may arise from
any of several distinct cognitive mechanisms including the
decay of a memory representation over time, the discounting
of learning magnitude by feedback delay of this
representation, or the discounting of learning in a
reinforcement-learning stimulus-response framework. In this
way, the models contribute the most important piece: they
pin down the process-level mechanism consistent with the
experiments.

\end{document}
